{"cells":[{"cell_type":"code","execution_count":null,"id":"f870e047-78a3-40b9-abda-29294c523ba3","metadata":{"tags":[],"id":"f870e047-78a3-40b9-abda-29294c523ba3"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy.integrate import solve_ivp\n","from scipy.optimize import minimize\n","import matplotlib.pyplot as plt\n","\n","# 1. Load data\n","def load_data(filename):\n","    df = pd.read_csv(filename, index_col=0)\n","    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n","    years = df.columns.astype(int)\n","    absolute_data = df.values\n","    proportions = absolute_data / absolute_data.sum(axis=0)\n","    return df.index.tolist(), years, proportions\n","\n","# 2. AS model\n","def extended_as_model(t, x, s, a, beta):\n","    n = len(x)\n","    dxdt = np.zeros(n)\n","    epsilon = 1e-12\n","    for i in range(n):\n","        sum_gain = 0\n","        sum_loss = 0\n","        xi = max(x[i], epsilon)\n","        one_minus_xi = max(1 - xi, epsilon)\n","        for j in range(n):\n","            if i != j:\n","                xj = max(x[j], epsilon)\n","                one_minus_xj = max(1 - xj, epsilon)\n","                Pji = s[i] * xi**beta * one_minus_xj**(a - beta)\n","                Pij = s[j] * xj**beta * one_minus_xi**(a - beta)\n","                sum_gain += x[j] * Pji\n","                sum_loss += Pij\n","        dxdt[i] = sum_gain - x[i] * sum_loss\n","    return dxdt\n","\n","# 3. Simulate with tiny yearly shifts\n","def simulate_language_dynamics(x0, s, a, beta, t_span, t_eval):\n","    if callable(s):\n","        s_values = np.array([s(t) for t in t_eval])\n","    else:\n","        s_values = np.tile(s, (len(t_eval), 1))\n","\n","    sol = solve_ivp(extended_as_model, t_span, x0, args=(s_values[0], a, beta),\n","                    t_eval=[t_span[0]], method='LSODA')\n","\n","    for i in range(1, len(t_eval)):\n","        sol_i = solve_ivp(extended_as_model, [t_eval[i-1], t_eval[i]], sol.y[:, -1],\n","                         args=(s_values[i], a, beta), t_eval=[t_eval[i]], method='LSODA')\n","        sol.y = np.hstack((sol.y, sol_i.y))\n","        sol.t = np.hstack((sol.t, sol_i.t))\n","\n","    return sol.y\n","\n","# 4. Low-volatility status shifts\n","def gradual_status(t, base_s, volatility=0.01, mean_reversion=0.1):\n","    \"\"\"Tiny annual shifts with mean reversion to original status\"\"\"\n","    if not hasattr(gradual_status, 'last_s'):\n","        gradual_status.last_s = base_s.copy()\n","\n","    # Tiny random changes (volatility=0.01 is 1% max change)\n","    delta = volatility * (np.random.random(len(base_s)) - 0.5)  # Centered at 0\n","\n","    # Mean reversion pulls status back toward original values\n","    gradual_status.last_s += delta + mean_reversion * (base_s - gradual_status.last_s)\n","\n","    # Ensure valid probabilities\n","    gradual_status.last_s = np.clip(gradual_status.last_s, 0.001, 1.0)\n","    gradual_status.last_s /= np.sum(gradual_status.last_s)\n","\n","    return gradual_status.last_s\n","\n","# 5. New: Simulate factors (M, I, B)\n","def simulate_factors(n, t_eval, initial_values=None, growth_rates=None, volatility=0.05):\n","    \"\"\"\n","    Simulate M (migration), I (internet), B (business relations) factors\n","    Returns: dict of {'M': array, 'I': array, 'B': array} for each time point\n","    \"\"\"\n","    if initial_values is None:\n","        initial_values = {'M': np.random.rand(n),\n","                         'I': np.random.rand(n),\n","                         'B': np.random.rand(n)}\n","    if growth_rates is None:\n","        growth_rates = {'M': np.random.normal(0, 0.02, n),\n","                       'I': np.random.normal(0.03, 0.01, n),\n","                       'B': np.random.normal(0.02, 0.01, n)}\n","\n","    factors = {'M': np.zeros((len(t_eval), n)),\n","              'I': np.zeros((len(t_eval), n)),\n","              'B': np.zeros((len(t_eval), n))}\n","\n","    for i, t in enumerate(t_eval):\n","        if i == 0:\n","            for key in ['M', 'I', 'B']:\n","                factors[key][i] = initial_values[key]\n","        else:\n","            for key in ['M', 'I', 'B']:\n","                # Geometric Brownian motion-like simulation\n","                factors[key][i] = factors[key][i-1] * (1 + growth_rates[key] +\n","                                                      np.random.normal(0, volatility, n))\n","                # Ensure values stay between 0 and 1\n","                factors[key][i] = np.clip(factors[key][i], 0.01, 1.0)\n","\n","    return factors\n","\n","# 6. New: Enhanced dynamic status with multiple factors and gradual shifts\n","def enhanced_dynamic_status(t, s_base, gdp_df, lang_to_countries, factors, weights):\n","    \"\"\"\n","    Calculate status using formula: si = a1*si_base + a2*Mi + a3*Ei + a4*Ii + a5*Bi\n","    weights = [a1, a2, a3, a4, a5] (should sum to 1)\n","    \"\"\"\n","    # Get economic factor\n","    Ei = compute_ei_t(gdp_df, lang_to_countries, t)\n","\n","    # Find index for current time\n","    t_index = np.where(factors['time'] == t)[0][0]\n","\n","    # Get current factors\n","    Mi = factors['M'][t_index]\n","    Ii = factors['I'][t_index]\n","    Bi = factors['B'][t_index]\n","\n","    # Normalize all factors (except Ei which is already normalized)\n","    Mi = Mi / Mi.sum()\n","    Ii = Ii / Ii.sum()\n","    Bi = Bi / Bi.sum()\n","\n","    # Calculate combined status\n","    s = (weights[0] * s_base +\n","         weights[1] * Mi +\n","         weights[2] * Ei +\n","         weights[3] * Ii +\n","         weights[4] * Bi)\n","\n","    # Apply gradual status shifts to the combined status\n","    s = gradual_status(t, s, volatility=0.005, mean_reversion=0.05)\n","\n","    return s\n","\n","# 7. Loss function\n","def loss_function_with_si(params, x_data, t_eval):\n","    a, beta = params[:2]\n","    s = params[2:]\n","    s = np.clip(s, 1e-6, 1.0)  # keep s values within [0, 1] to avoid instability\n","    s = s / np.sum(s)  # normalize so s behaves like probabilities\n","\n","    x0 = x_data[:, 0]\n","    x_model = simulate_language_dynamics(x0, s, a, beta, (t_eval[0], t_eval[-1]), t_eval)\n","    return np.mean(np.abs(x_model - x_data))\n","\n","# 8. Fit model\n","def fit_model_per_language(x_data, t_eval):\n","    n = x_data.shape[0]\n","    initial_s = np.ones(n) / n\n","    initial_params = np.concatenate(([1.0, 0.5], initial_s))  # [a, beta, s1, ..., sn]\n","\n","    bounds = [(0.01, 3.0), (0.01, 3.0)] + [(1e-6, 1.0)] * n  # bounds for a, beta, s_i\n","\n","    result = minimize(loss_function_with_si, initial_params, args=(x_data, t_eval),\n","                      bounds=bounds, method='L-BFGS-B')\n","\n","    a_fit, beta_fit = result.x[:2]\n","    s_fit = result.x[2:]\n","    s_fit /= np.sum(s_fit)  # ensure s is normalized\n","    print(f\"Optimization success: {result.success}, Loss: {result.fun}\")\n","    print(f\"a = {a_fit:.4f}, beta = {beta_fit:.4f}\")\n","    print(\"s =\", s_fit)\n","    return a_fit, beta_fit, s_fit\n","\n","# 9. Economic factor calculation\n","gdp_df = pd.read_csv(\"gdp_by_country_2025_2075.csv\", index_col=0)\n","\n","#language -> (country, weight)\n","lang_to_countries = {\n","  'English': [('United States',1.0), ('United Kingdom',1.0)],\n","  'Mandarin Chinese': [('China',1.0), ('Taiwan',1.0)],\n","  'Hindi': [('India',0.4), ('Nepal',1.0)],\n","  'Spanish': [('Spain',1.0), ('Mexico',1.0)],\n","  'French': [('France',1.0), ('Canada',0.2)],\n","  'Modern Arabic': [('Saudi Arabia',1.0), ('Egypt',1.0)],\n","  'Bengali': [('India',0.1), ('Bangladesh',1.0)],\n","  'Russian': [('Russia',1.0)],\n","  'Portuguese': [('Brazil',1.0), ('Portugal',1.0)],\n","  'Indonesian': [('Indonesia',1.0)],\n","  'Urdu': [('India',0.05), ('Pakistan',1.0)],\n","  'Japanese': [('Japan',1.0)],\n","  'German': [('Germany',1.0), ('Switzerland',0.6)],\n","}\n","languages = [\n","    'English',\n","    'Mandarin Chinese',\n","    'Hindi',\n","    'Spanish',\n","    'French',\n","    'Modern Arabic',\n","    'Bengali',\n","    'Russian',\n","    'Portuguese',\n","    'Indonesian',\n","    'Urdu',\n","    'Japanese',\n","    'German'\n","]\n","\n","def compute_ei_t(gdp_df, lang_to_countries, t):\n","    Ei = []\n","    for lang in languages:\n","        total = 0\n","        for country, weight in lang_to_countries[lang]:\n","            if country in gdp_df.columns:\n","                total += weight * gdp_df.loc[t, country]\n","        Ei.append(total)\n","    Ei = np.array(Ei)\n","    return Ei / Ei.sum()  # normalize\n","\n","# 10. Updated main function\n","def main(filename):\n","    languages, years, proportions = load_data(filename)\n","    recent_years = years[years >= 2015]\n","    recent_data = proportions[:, years >= 2015]\n","    a_fit, beta_fit, s_fit = fit_model_per_language(recent_data, recent_years)\n","\n","    forecast_start = years[-1]\n","    future_years = np.arange(forecast_start, forecast_start + 50, 1)\n","\n","    # Simulate factors for future years\n","    factors = simulate_factors(len(languages), future_years)\n","    factors['time'] = future_years  # Add time to factors dict for reference\n","\n","    # Define weights for status calculation [a1, a2, a3, a4, a5]\n","    # These weights should sum to 1. Adjust based on your assumptions.\n","    status_weights = [0.4, 0.18, 0.18, 0.1, 0.14]  # Base, Migration, Economic, Internet, Business\n","\n","    # Create status function with all factors and gradual shifts\n","    s_enhanced = lambda t: enhanced_dynamic_status(\n","        t, s_fit, gdp_df, lang_to_countries, factors, status_weights\n","    )\n","\n","    # Single simulation\n","    x0 = proportions[:, -1]\n","    x_forecast = simulate_language_dynamics(\n","        x0, s_enhanced, a_fit, beta_fit,\n","        (forecast_start, forecast_start + 50),\n","        future_years\n","    )\n","\n","    plt.figure(figsize=(12, 6))\n","    for i, lang in enumerate(languages):\n","        plt.plot(future_years, x_forecast[i], label=lang, alpha=0.8)\n","    plt.title('Language Projections with Enhanced Dynamic Status and Gradual Shifts')\n","    plt.xlabel('Year')\n","    plt.ylabel('Fraction of Speakers')\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","    plt.grid(True, linestyle=':', alpha=0.5)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Uncertainty analysis (10 runs)\n","    n_simulations = 10\n","    all_simulations = np.zeros((n_simulations, len(languages), len(future_years)))\n","\n","    for sim in range(n_simulations):\n","        # Resimulate factors for each run to capture uncertainty\n","        sim_factors = simulate_factors(len(languages), future_years)\n","        sim_factors['time'] = future_years\n","        s_sim = lambda t: enhanced_dynamic_status(\n","            t, s_fit, gdp_df, lang_to_countries, sim_factors, status_weights\n","        )\n","\n","        all_simulations[sim] = simulate_language_dynamics(\n","            x0, s_sim, a_fit, beta_fit,\n","            (forecast_start, forecast_start + 50),\n","            future_years\n","        )\n","\n","    # Plot median and 25-75th percentiles\n","    plt.figure(figsize=(12, 6))\n","    for i, lang in enumerate(languages[:3]):  # First 3 languages\n","        median = np.median(all_simulations[:, i, :], axis=0)\n","        p25 = np.percentile(all_simulations[:, i, :], 25, axis=0)\n","        p75 = np.percentile(all_simulations[:, i, :], 75, axis=0)\n","        plt.plot(future_years, median, label=lang, lw=2)\n","        plt.fill_between(future_years, p25, p75, alpha=0.15)\n","\n","    plt.title('Projections with 25-75th Percentile Bands (Enhanced Status with Gradual Shifts)')\n","    plt.xlabel('Year')\n","    plt.ylabel('Fraction of Speakers')\n","    plt.legend()\n","    plt.show()\n","\n","main(\"language_speakers_3.csv\")"]},{"cell_type":"code","execution_count":null,"id":"5e5de3de-2b43-41aa-b615-c813af665f61","metadata":{"id":"5e5de3de-2b43-41aa-b615-c813af665f61"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}